---
title: "Practical Machine Learning Project"
author: "Jacques Chatard"
date: "April 4 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Purpose of this project  and data source

**In this project we will use and compare 3 different models on WLE (weight lifting exercises) dataset.**  

1- A Boosted trees model 'modlgbm'.  
2- A Linear discriminant analysis model 'modllda'.  
3- A Random forest model 'modlrf'.     

**And we will try to estimate the accuracy of the predictions on the  'classe'** 
**variable which classifies the quality of execution of weight lifting movements.**

This project is related to the human activity recognition 'HAR'.
We will use the WLE dataset collected by :   

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative 
Activity Recognition of Weight Lifting Exercises. Proceedings of 4th 
International Conference in Cooperation with SIGCHI (Augmented Human '13) . 
Stuttgart, Germany: ACM SIGCHI, 2013.  
Many thanks for their generosity.   

### Url about WLE data set:  
 
[more info](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises)

### Rmd file of this page :  

[Github rmd file:](https://github.com/chatard/PML-Coursera-Project/blob/gh-pages/assignment.Rmd)  

# Loading and reading data

- Loading data: 

```{r Loadingcsv1}
destfile="pml-training.csv"
fileURL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
if (!file.exists(destfile)) {
        download.file(fileURL ,destfile,method="auto")
}

```
```{r Loadingcsv2}
destfile="pml-testing.csv"
fileURL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if (!file.exists(destfile)) {
        download.file(fileURL ,destfile,method="auto")
}
```
- Reading data: 
```{r readingdata}
pml_training<-read.table(file = "pml-training.csv", header=TRUE, sep=",")
pml_testing <-read.table(file ="pml-testing.csv", header=TRUE, sep=",")
dim(pml_training); dim(pml_testing)

```

# loading the required libraries:
```{r libraries, message=FALSE}
library(caret); library(kernlab); library(randomForest); library(dplyr)
library(parallel); library(foreach); library(doParallel); library(lattice)

```

The original data is already divided into two groups of data: pml-training and pml-testing.
Nevertheless, pml-testing represents only 1 per thousand of the total data.
I would proceed to a new subdivision of the data from pml-training.

# Cleaning the data:  

To get clean data, I will use the original data that comes from 'pml_training'.

## Near-zero variance predictors:
1 . I will remove them from the data.  

2 . checking if outcome 'classe' variable has not disappeared.

```{r nearzerovar}

x = nearZeroVar(pml_training)
newBaseData <- pml_training[, -x]
dim(newBaseData)
#checking if outcome 'classe' variable has not disappeared :
"classe" %in% names(newBaseData)

```

## Also, I will remove column of variables with na values:
```{r removingNa}
NA_cols <- sapply(newBaseData, function(x) sum(is.na(x))) >0
newBaseData<-newBaseData[, NA_cols==FALSE]
dim(newBaseData)
```

## And finally, I will remove the first 6 columns of variables:    

These variables relate to the identification of participants and the time periods in which observations were recorded and these variables are not predictor variables.

 
```{r remove6firstcols}
newBaseData <- newBaseData[, -(1:6)]
dim(newBaseData)

```


#Splitting:   cleaned data partition:   

```{r datapartition}

inTrain <- createDataPartition(newBaseData$classe, p =0.70, list = FALSE)
training <- newBaseData[inTrain,]
testing <- newBaseData[-inTrain,]
dim(training);dim(testing)

```

# Defining three models  

## Before definining models we have to define training control:

In order to optimize the training phase to the RAM resources available 
I have to find a suitable configuration. So  I configure a 'traincontrol' object:  
I  choose a 5-fold for  cross-validation:   

```{r traincontrolSettings}
set.seed(1234)
Ctrl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)
```

## Models:

Each of the 3 models will use the named variable **"classe" as outcome variable**.
It is the criterion of judgment of the good execution of the weight lifting.
'A' corresponds to a correctly performed movement.
The 4 other classes are a classification of specific mistakes in the realization of the movement.  


**1- Boosted trees model:**  

```{r gbmModel}

# Boosted trees model:

set.seed(1234)
modlgbm <- train(classe ~ ., data= training, method="gbm", trControl=Ctrl, verbose = FALSE)
#modlgbm

```

**2- Linear discriminant analysis model:**    

```{r ldaModel}

# Linear discriminant analysis model: 
set.seed(1234)
modllda <- train(classe ~ ., data =training, method="lda", trControl=Ctrl, verbose = FALSE)
#modllda

```



**3- Random Forest model:**  


```{r rfModel }

# Random Forest model:
set.seed(1234)
modlrf <- train(classe ~., method="rf", data=training, trControl=Ctrl, verbose = FALSE)
#modlrf
```



##Variable importance:  

Finding the most important predictors:
Here I use the caret package function varimp() to find most important predictors 
with a specific model (random forest).

And I plot the Top 20 predictors.

```{r varimp }
varImp(modlrf)
plot(varImp(modlrf),top=20)

```

```{r rfFinalmodel}
#modlrf$finalModel
```
# Comparison of the 3 models:

### Out of sample error:  

It's important to have an idea of 'out sample error'
for each of the models. For that we will make the estimate on a resampling.

```{r modelsComparison}
results <- resamples(list(GBM=modlgbm, LDA=modllda, RF=modlrf))
summary(results)
```


```{r resamplingresults}
RFoutSample<-1-mean(results$values[, "RF~Accuracy"])
GBMoutSample<-1-mean(results$values[, "GBM~Accuracy"])
LDAoutSample<-1-mean(results$values[, "LDA~Accuracy"])

```

| model 	|  out of sample error 	|
|-------	|----------------------	|
| RF    	|  `r RFoutSample`    	|
| GBM   	| `r GBMoutSample`     	|
| LDA   	| `r LDAoutSample`     	|


# Model selection:


From this comparison, it follows that the best model is: random forest model.
So this is the one I will choose.


## Sampling in training
```{r predontraining}
trainingSample = predict(modlrf, training)
confusionMatrix(training$classe, trainingSample)
```

##Sampling in testing

```{r predontesting}
testingSample = predict(modlrf, testing)
confusionMatrix(testing$classe,testingSample)
```

# Predicting Classe of Testing Data Set

Utilize the prediction model on the testing data set.

```{r RFpredonpml_testing}
predictRF <- predict(modlrf, newdata=pml_testing)
predictRF
```




















